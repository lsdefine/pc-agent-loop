import os, sys
if sys.stdout is None: sys.stdout = open(os.devnull, "w")
if sys.stderr is None: sys.stderr = open(os.devnull, "w")
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))


import streamlit as st
import time, json, re

with open('tools_schema.json', 'r', encoding='utf-8') as f:
    TOOLS_SCHEMA = json.load(f)


st.set_page_config(page_title="Cowork", layout="wide")

from sidercall import SiderLLMSession, LLMSession, ToolClient
from agent_loop import agent_runner_loop, StepOutcome, BaseHandler

@st.cache_resource
def init():
    if not os.path.exists('temp'): os.makedirs('temp')
    mainllm = SiderLLMSession(multiturns=6)
    llmclient = ToolClient(mainllm.ask, auto_save_tokens=True)
    return llmclient

llmclient = init()

from ga import GenericAgentHandler, smart_format

def get_system_prompt():
    if not os.path.exists('memory'): os.makedirs('memory')
    if not os.path.exists('memory/global_mem.txt'):
        with open('memory/global_mem.txt', 'w', encoding='utf-8') as f: f.write('')
    if not os.path.exists('memory/global_mem_insight.txt'):
        with open('memory/global_mem_insight.txt', 'w', encoding='utf-8') as f:
            f.write('PATHS: ../memory/global_mem.txt (Facts), ../memory/global_mem_insight.txt (Logic), ../ (Code Root).')
    with open('sys_prompt.txt', 'r', encoding='utf-8') as f:
        prompt = f.read()
    try:
        with open('memory/global_mem_insight.txt', 'r', encoding='utf-8') as f: 
            insight = f.read()
        prompt += f"\n\n[Global Memory Insight]\n{insight}"
    except FileNotFoundError: pass
    return prompt

if "last_goal" not in st.session_state:
    st.session_state.last_goal = ""

def refine_user_goal(raw_query, last_goal):
    """é€šè¿‡ LLM æç‚¼ç”¨æˆ·çœŸå®æ„å›¾"""
    if not last_goal:
        return raw_query

    decide_prompt = f"""
ç”¨æˆ·ä¹‹å‰çš„ç›®æ ‡æ˜¯: "{last_goal}"
ç”¨æˆ·ç°åœ¨è¾“å…¥äº†: "{raw_query}"

è¯·åˆ¤æ–­ï¼š
1. å¦‚æœç”¨æˆ·æä¾›è¡¥å……ä¿¡æ¯ã€æˆ–è€…æ˜¯æ¥ç»­ä¹‹å‰çš„ä»»åŠ¡ï¼Œè¯·è¾“å‡ºåˆå¹¶åçš„ã€æœ€ç»ˆç›®æ ‡ã€‘ã€‚
2. å¦‚æœç”¨æˆ·åªæ˜¯æŒ‡å‡ºä¹‹å‰åšæ³•æœ‰é”™è€Œéå˜æ›´ç›®æ ‡ï¼Œé‚£ä¹ˆè¯·è¾“å‡ºåŸç›®æ ‡ä¸åšä¿®æ”¹ã€‚
3. å¦‚æœç”¨æˆ·å¼€å¯äº†ä¸€ä¸ªå®Œå…¨ä¸ç›¸å…³çš„æ–°è¯é¢˜ï¼Œè¯·ç›´æ¥è¾“å‡ºç”¨æˆ·ç°åœ¨çš„è¾“å…¥å†…å®¹ã€‚

è¯·ç›´æ¥è¾“å‡ºç›®æ ‡æè¿°ï¼Œä¸è¦åŒ…å«ä»»ä½•å¤šä½™çš„æ–‡å­—ã€è§£é‡Šæˆ–æ ‡ç‚¹ã€‚
"""
    try:
        refined = llmclient.llm_func(decide_prompt).strip()
        return refined if refined else raw_query
    except:
        return raw_query

def agent_backend_stream(raw_query):
    #final_goal = refine_user_goal(raw_query, st.session_state.last_goal)
    #if final_goal != raw_query: yield f"[Goal Refined] {final_goal}\n"

    history = st.session_state.get("last_history", [])
    rquery = smart_format(raw_query.replace('\n', ' '))
    history.append(f"[USER]: {rquery}")

    sys_prompt = get_system_prompt()
    handler = GenericAgentHandler(None, history, './temp')
    llmclient.last_tools = ''   
    ret = yield from agent_runner_loop(llmclient,
        sys_prompt, raw_query, handler,
        TOOLS_SCHEMA, max_turns=25)
    #st.session_state.last_goal = final_goal
    st.session_state.last_history = handler.history_info
    return ret

st.title("ğŸ–¥ï¸ Cowork")

if "messages" not in st.session_state:
    st.session_state.messages = []

for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

if prompt := st.chat_input("è¯·è¾“å…¥æŒ‡ä»¤"):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        full_response = ""
        for chunk in agent_backend_stream(prompt):
            full_response += chunk
            message_placeholder.markdown(full_response + "â–Œ")
        message_placeholder.markdown(full_response)
    st.session_state.messages.append({"role": "assistant", "content": full_response})