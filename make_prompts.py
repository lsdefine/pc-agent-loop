import sys, os, re
import pyperclip
import json, time
from pathlib import Path
import subprocess
import tempfile
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
from sidercall import SiderLLMSession, LLMSession, ToolClient


ask = SiderLLMSession().ask


def generate_tool_schema():
    """
    é€šè¿‡ä»£ç å†…çœï¼Œå°† Handler çš„é€»è¾‘æ˜ å°„ä¸ºé«˜è¯­ä¹‰çš„å·¥å…·æè¿°ã€‚
    """
    with open('ga.py', 'r', encoding='utf-8') as f:
        ga_code = f.read()
    # æç®€ä¸”å…·å¤‡é«˜åº¦æ¦‚æ‹¬èƒ½åŠ›çš„å…ƒ Prompt
    meta_prompt = f"""
# Role
ä½ æ˜¯ä¸€ä¸ªå…·å¤‡æ·±åº¦æ¨ç†èƒ½åŠ›çš„ AI ç³»ç»Ÿæ¶æ„å¸ˆã€‚ä½ å°†é€šè¿‡é˜…è¯» `GenericAgentHandler` æºç ï¼Œæ„å»ºå…¶å¯¹åº”çš„å·¥å…·èƒ½åŠ›çŸ©é˜µã€‚

# Task
åˆ†æä¸‹æ–¹çš„æºç ï¼Œå¹¶è¾“å‡º OpenAI Tool Schemaã€‚åœ¨è¾“å‡º JSON ä¹‹å‰ï¼Œä½ å¿…é¡»è¿›è¡Œå†…éƒ¨æ€è€ƒï¼ˆThinking Processï¼‰ã€‚

# Thinking Process Requirements
åœ¨ `<thinking>` æ ‡ç­¾ä¸­ï¼Œè¯·æŒ‰é¡ºåºåˆ†æï¼š
1. **æ ¸å¿ƒå·¥å…·é“¾è¯†åˆ«**ï¼šè¯†åˆ«æ‰€æœ‰ `do_xxx` æ–¹æ³•ï¼Œå¹¶åˆ†æå®ƒä»¬ä¾èµ–çš„åº•å±‚ Utility å‡½æ•°ã€‚
2. **å†…å®¹æº¯æºå®¡è®¡**ï¼šé‡ç‚¹åˆ†æå“ªäº›å·¥å…·æ˜¯ä» `response.content` æå–æ ¸å¿ƒé€»è¾‘ï¼ˆå¦‚ä»£ç å—ï¼‰çš„ã€‚å¯¹äºè¿™äº›å·¥å…·ï¼Œç¡®è®¤åœ¨ Schema å‚æ•°ä¸­æ’é™¤æ‰å¯¹åº”çš„å­—æ®µã€‚
3. **è°ƒç”¨ç­–ç•¥æ¨å¯¼**ï¼šåˆ†æå·¥å…·é—´çš„åä½œå…³ç³»ï¼ˆä¾‹å¦‚ `file_read` å¦‚ä½•ä¸º `file_patch` æä¾›å®šä½ï¼‰ã€‚
4. **å…œåº•é€»è¾‘ç¡®è®¤**ï¼šæ˜ç¡®æŸäº›ç‰¹æ®Šä¸‡èƒ½å·¥å…·åœ¨ç³»ç»Ÿä¸­çš„ä¿åº•è§’è‰²ï¼Œå¿«é€Ÿå·¥å…·æ— æ³•æ‰§è¡Œçš„æ“ä½œç”±ä¿åº•å·¥å…·æ‰§è¡Œï¼Œä½†æ­£å¸¸åº”ä¼˜å…ˆä½¿ç”¨æ–¹ä¾¿çš„å·¥å…·ã€‚
5. **æ³¨é‡Šå®¡é˜…**ï¼šç»“åˆå‡½æ•°æ³¨é‡Šï¼Œç†è§£æ¯ä¸ªå·¥å…·çš„ä½¿ç”¨é™åˆ¶ï¼Œå…¶ä¸­çš„é‡è¦ä¿¡æ¯åŠ¡å¿…åæ˜ åœ¨å·¥å…·æè¿°ä¸­ï¼ˆå¦‚é•¿åº¦é™åˆ¶ç­‰ï¼‰ã€‚
æ³¨é‡Šä¸­çš„é‡è¦ä¿¡æ¯åŠ¡å¿…åæ˜ åœ¨å·¥å…·æè¿°ä¸­ã€‚
æ³¨é‡Šä¸­çš„é‡è¦ä¿¡æ¯åŠ¡å¿…åæ˜ åœ¨å·¥å…·æè¿°ä¸­ã€‚

# Tool Schema Formatting Rules
- **å‚æ•°å¯¹é½**ï¼šä»…åŒ…å« `do_xxx` æ–¹æ³•ä¸­é€šè¿‡ `args.get()` æ˜¾å¼è·å–çš„å‚æ•°ã€‚
- **é«˜å¼•å¯¼æ€§æè¿°**ï¼šæè¿°åº”åŒ…å«â€œä½•æ—¶è°ƒç”¨â€ä»¥åŠâ€œå¦‚ä½•æ ¹æ®åé¦ˆä¿®æ­£â€ï¼Œéœ€è¦æ³¨æ„å‡½æ•°çš„æ³¨é‡Šäº‹é¡¹ã€‚
- **è¾“å‡ºæ ¼å¼**ï¼šå…ˆè¾“å‡º `<thinking>` å—ï¼Œç„¶åè¾“å‡º ```json å—ã€‚

# Source Code
{ga_code}

# Output
è¯·å¼€å§‹æ€è€ƒå¹¶ç”Ÿæˆï¼š
"""
    
    # å‡è®¾ ask æ˜¯ä½ å·²ç»å°è£…å¥½çš„ LLM è°ƒç”¨æ¥å£
    raw_response = ask(meta_prompt, model="gemini-3.0-flash")
    print(raw_response)
    
    # --- å¥å£®çš„ JSON è§£æé€»è¾‘ ---
    try:
        # 1. æ¸…é™¤ Markdown å›´æ 
        clean_json = raw_response.strip()
        if clean_json.startswith("```"):
            # å…¼å®¹ ```json å’Œ ``` 
            clean_json = re.sub(r'^```(?:json)?\s*', '', clean_json)
            clean_json = re.sub(r'\s*```$', '', clean_json)
        
        # 2. ç§»é™¤å¯èƒ½çš„é JSON å‰å¯¼/åç¼€æ–‡å­—ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
        start_idx = clean_json.find('[')
        end_idx = clean_json.rfind(']') + 1
        if start_idx != -1 and end_idx != -1:
            clean_json = clean_json[start_idx:end_idx]
            
        final_schema = json.loads(clean_json)
        
        if final_schema:
            with open('tools_schema.json', 'w', encoding='utf-8') as f:
                json.dump(final_schema, f, indent=2, ensure_ascii=False)
            print("âœ… æˆåŠŸä»ä»£ç å†…çœç”Ÿæˆ Schema å¹¶æŒä¹…åŒ–ã€‚")
        return final_schema

    except Exception as e:
        print(f"âŒ è§£æ Schema å¤±è´¥: {e}\nåŸå§‹å“åº”: {raw_response}")
        return None


def make_system_prompt(ga_code_path='ga.py'):
    with open(ga_code_path, 'r', encoding='utf-8') as f:
        ga_code = f.read()

    # è¿™ä¸ªå…ƒ Prompt çš„ç›®æ ‡æ˜¯ç”Ÿæˆâ€œä¸–ç•Œè§‚â€è€Œéâ€œè¯´æ˜ä¹¦â€
    meta_prompt = f"""
# Role
ä½ æ˜¯ä¸€ä¸ª AI æ¶æ„å¸ˆã€‚è¯·é˜…è¯»ä¸‹æ–¹çš„å·¥å…·åº“æºç ï¼Œä¸º Agent ç”Ÿæˆä¸€ä»½ã€ç³»ç»Ÿçº§è®¤çŸ¥æŒ‡ä»¤ (System Prompt)ã€‘ã€‚

# Task
åŸºäºä»£ç é€»è¾‘ï¼Œå®šä¹‰ Agent çš„â€œèƒ½åŠ›è¾¹ç•Œâ€å’Œâ€œè¡ŒåŠ¨åè®®â€ã€‚

# Requirements (Crucial)
1. **æ‰“ç ´é¢„è®­ç»ƒåè§**ï¼šé’ˆå¯¹æ¨¡å‹å¸¸è¯´çš„â€œæˆ‘åªæ˜¯ AIï¼Œä¸èƒ½æ“ä½œç½‘é¡µ/æ–‡ä»¶â€è¿›è¡Œä¿®æ­£ã€‚æ˜ç¡®å‘Šè¯‰å®ƒï¼šä½ ç°åœ¨æ‹¥æœ‰ç‰©ç†æ“ä½œæƒé™ï¼Œå·¥å…·è®¾è®¡ä¿è¯äº†æ‰€æœ‰æƒé™åšæ‰€æœ‰äº‹æƒ…ã€‚
2. **é¿å¼€å†—ä½™**ï¼šä¸è¦é‡å¤ Tool Schema é‡Œçš„å‚æ•°ç»†èŠ‚ã€‚
3. **èƒ½åŠ›è¾¹ç•Œå®šä¹‰**ï¼š
   - ç½‘é¡µæ“ä½œï¼šå®ƒä¸æ˜¯é€šè¿‡â€œæƒ³è±¡â€ä¸Šç½‘ï¼Œè€Œæ˜¯é€šè¿‡å®æ—¶çš„æµè§ˆå™¨è¯»å†™ã€‚
   - æ–‡ä»¶æ“ä½œï¼šå®ƒæ‹¥æœ‰ç‰©ç†æ–‡ä»¶è¯»å†™æƒé™ï¼Œä¸”éµå¾ªâ€œå…ˆè¯»åå†™â€çš„ç¨³å¥æ€§åŸåˆ™ã€‚
   - ä¿åº•é€»è¾‘ï¼šå½“ä¸“ç”¨å·¥å…·å¤±æ•ˆæ—¶ï¼Œä½¿ç”¨ `code_run` ç¼–å†™è„šæœ¬è§£å†³ä¸€åˆ‡ã€‚
   - ç‰¹æ®Šçš„update_planï¼ˆä»…åœ¨å¤æ‚ä»»åŠ¡æ—¶ä½¿ç”¨ï¼‰å’Œask_userï¼ˆç”¨æˆ·ä¹Ÿæ˜¯æœ‰æ•ˆèµ„æºï¼‰å·¥å…·ã€‚
4. **è¡ŒåŠ¨åè®®**ï¼š
   - å¿…é¡»åœ¨è¡ŒåŠ¨å‰è¿›è¡Œ<thinking>

æˆ‘åé¢è¿˜ä¼šé™„ä¸Šå…·ä½“çš„å·¥å…·æè¿°å’ŒSchemaï¼Œæ‰€ä»¥ä¸è¦é‡å¤ã€‚
ä¸»è¦ä»¥ä¸–ç•Œè§‚ä¸ºä¸»ï¼Œä¸è¦çº ç»“äºå…·ä½“å·¥å…·ã€‚

# Input Source Code
{ga_code}

# Output
ä»…è¾“å‡º System Prompt çš„æ­£æ–‡ï¼Œè¯­æ°”è¦æœæ–­ã€æŒ‡ä»¤åŒ–ã€‚
"""
    print("ğŸ§  æ­£åœ¨é‡å¡‘ Agent ä¸–ç•Œè§‚ (Generating System Prompt)...")
    # è°ƒç”¨ä½ çš„ llmclient.ask
    system_prompt_content = ask(meta_prompt)
    print("ğŸ“ ç”Ÿæˆçš„ System Prompt å†…å®¹å¦‚ä¸‹ï¼š\n")
    print(system_prompt_content)
    clean_content = re.sub(r'<[^>]+>', '', system_prompt_content)
    with open('sys_prompt.txt', 'w', encoding='utf-8') as f:
        f.write(clean_content)
    return clean_content

# --- ä¸»é€»è¾‘ ---
if __name__ == "__main__":
    if len(sys.argv) < 2:
        print("Usage: python make_prompts.py [schema|prompt]")
        sys.exit(1)
    
    cmd = sys.argv[1].lower()
    if cmd == "schema":
        generate_tool_schema()
    elif cmd == "prompt":
        make_system_prompt()
    else:
        print(f"Unknown command: {cmd}")
        print("Available commands: schema, prompt")