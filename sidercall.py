import os, json, re, time, requests
from sider_ai_api import Session

try:
    from mykey import sider_cookie, capikey
except ImportError:
    sider_cookie = ""
    capikey = ""

class SiderLLMSession:
    def __init__(self, multiturns=6):
        self._core = Session(cookie=sider_cookie, proxies={'https':'127.0.0.1:2082'})   
    def ask(self, prompt, model="gemini-3.0-flash"):
        if len(prompt) > 30000: prompt = prompt[-29500:]
        return ''.join(self._core.chat(prompt, model))
  
class LLMSession:
    def __init__(self, api_key=capikey, api_base="http://113.45.39.247:3001/v1", multiturns=6):
        self.api_key = api_key
        self.api_base = api_base
        self.messages = []
        self.multiturns = multiturns
        
    def ask(self, prompt, model="openai/gpt-5.1"):
        self.messages.append({"role": "user", "content": prompt})
        if len(self.messages) > self.multiturns:
            self.messages = self.messages[-self.multiturns:]
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        try:
            response = requests.post(
                f"{self.api_base}/chat/completions",
                headers=headers,
                json={
                    "model": model, 
                    "messages": self.messages, 
                    "temperature": 0.5
                },
                timeout=60
            )
            res_json = response.json()
            content = res_json["choices"][0]["message"]["content"]
            self.messages.append({"role": "assistant", "content": content})
            return content
        except Exception as e:
            return f"Error: {str(e)}"

class MockFunction:
    def __init__(self, name, arguments):
        self.name = name
        self.arguments = arguments  

class MockToolCall:
    def __init__(self, name, args):
        arg_str = json.dumps(args, ensure_ascii=False) if isinstance(args, dict) else args
        self.function = MockFunction(name, arg_str)

class MockResponse:
    def __init__(self, thinking, content, tool_calls, raw):
        self.thinking = thinking        # å­˜æ”¾ <thinking> å†…éƒ¨çš„æ€ç»´è¿‡ç¨‹
        self.content = content          # å­˜æ”¾å»é™¤æ ‡ç­¾åçš„çº¯æ–‡æœ¬å›å¤
        self.tool_calls = tool_calls    # å­˜æ”¾ MockToolCall åˆ—è¡¨ æˆ– None
        self.raw = raw
    def __repr__(self):    
        return f"<MockResponse thinking={bool(self.thinking)}, content='{self.content}', tools={bool(self.tool_calls)}>"

class ToolClient:
    def __init__(self, raw_api_func, auto_save_tokens=False):
        self.raw_api = raw_api_func
        self.auto_save_tokens = auto_save_tokens
        self.last_tools = ''
        self.total_cd_tokens = 0

    def chat(self, messages, tools=None):
        full_prompt = self._build_protocol_prompt(messages, tools)      
        print("Full prompt length:", len(full_prompt))
        raw_text = self.raw_api(full_prompt)
        with open('model_responses.txt', 'a', encoding='utf-8', errors="replace") as f:
            f.write(f"=== Prompt ===\n{full_prompt}\n=== Response ===\n{raw_text}\n\n")
        return self._parse_mixed_response(raw_text)

    def _build_protocol_prompt(self, messages, tools):
        system_content = next((m['content'] for m in messages if m['role'].lower() == 'system'), "ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ã€‚")
        history_msgs = [m for m in messages if m['role'].lower() != 'system']
        
        # æ„é€ å·¥å…·æè¿°
        tool_instruction = ""
        if tools:
            tools_json = json.dumps(tools, ensure_ascii=False, indent=2)
            tool_instruction = f"""
### âš¡ï¸ äº¤äº’åè®® (å¿…é¡»ä¸¥æ ¼éµå®ˆ)
è¯·æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤æ€è€ƒå¹¶è¡ŒåŠ¨ï¼š
1. **æ€è€ƒ**: åœ¨ `<thinking>` æ ‡ç­¾ä¸­å…ˆè¿›è¡Œæ€è€ƒï¼Œåˆ†æç°çŠ¶å’Œç­–ç•¥ã€‚
2. **æ€»ç»“**: åœ¨ `<summary>` ä¸­è¾“å‡º*æä¸ºç®€çŸ­*çš„é«˜åº¦æ¦‚æ‹¬çš„å•è¡Œï¼ˆ<30å­—ï¼‰ç‰©ç†å¿«ç…§ï¼ŒåŒ…æ‹¬ä¸Šæ¬¡å·¥å…·è°ƒç”¨ç»“æœè·å–çš„æ–°ä¿¡æ¯+æœ¬æ¬¡å·¥å…·è°ƒç”¨æ„å›¾å’Œé¢„æœŸã€‚æ­¤å†…å®¹å°†è¿›å…¥é•¿æœŸå·¥ä½œè®°å¿†ï¼Œè®°å½•å…³é”®ä¿¡æ¯ï¼Œä¸¥ç¦è¾“å‡ºæ— å®é™…ä¿¡æ¯å¢é‡çš„æè¿°ã€‚
3. **è¡ŒåŠ¨**: å¦‚æœéœ€è¦è°ƒç”¨å·¥å…·ï¼Œè¯·ç´§æ¥ç€è¾“å‡ºä¸€ä¸ª **<tool_use>å—**ï¼Œç„¶åç»“æŸï¼Œæˆ‘ä¼šç¨åç»™ä½ è¿”å›<tool_result>å—ã€‚
   æ ¼å¼: ```<tool_use>\n{{"function": "å·¥å…·å", "arguments": {{å‚æ•°}}}}\n</tool_use>\n```

### ğŸ› ï¸ å¯ç”¨å·¥å…·åº“
{tools_json}
"""
            if self.auto_save_tokens and self.last_tools == tools_json:
                tool_instruction = "\n### äº¤äº’åè®®ä¿æŒä¸å˜ï¼Œæ²¿ç”¨ä¹‹å‰çš„åè®®å’Œå·¥å…·åº“ã€‚\n"
            else:
                self.total_cd_tokens = 0
            self.last_tools = tools_json
            
        prompt = f"=== SYSTEM ===\n{system_content}\n{tool_instruction}\n\n"
        for m in history_msgs:
            role = "USER" if m['role'] == 'user' else "ASSISTANT"
            prompt += f"=== {role} ===\n{m['content']}\n\n"
            self.total_cd_tokens += len(m['content'])
            
        if self.total_cd_tokens > 9000: self.last_tools = ''

        prompt += "=== ASSISTANT ===\n" 
        return prompt

    def _parse_mixed_response(self, text):
        remaining_text = text
        thinking = ''
        think_pattern = r"<thinking>(.*?)</thinking>"
        think_match = re.search(think_pattern, text, re.DOTALL)
        
        if think_match:
            thinking = think_match.group(1).strip()
            remaining_text = re.sub(think_pattern, "", remaining_text, flags=re.DOTALL)
        
        tool_calls = None
        tool_pattern = r"<tool_use>(.*?)</tool_use>"
        tool_match = re.search(tool_pattern, text, re.DOTALL)
        
        json_str = ""
        if tool_match:
            json_str = tool_match.group(1).strip()
            remaining_text = re.sub(tool_pattern, "", remaining_text, flags=re.DOTALL)
        elif '<tool_use>' in remaining_text:
            weaktoolstr = remaining_text.split('<tool_use>')[-1].strip()
            json_str = weaktoolstr if weaktoolstr.endswith('}') else ''
            remaining_text = remaining_text.replace('<tool_use>'+weaktoolstr, "")

        if json_str:
            try:
                data = tryparse(json_str)
                func_name = data.get('function') or data.get('tool')
                args = data.get('arguments') or data.get('args')
                if args is None: args = {}
                if func_name: tool_calls = [MockToolCall(func_name, args)]
            except json.JSONDecodeError:
                print("[Warn] Failed to parse tool_use JSON:", json_str)
                thinking += f"[Warn] JSON è§£æå¤±è´¥ï¼Œæ¨¡å‹è¾“å‡ºäº†æ— æ•ˆçš„ JSON."
            except Exception as e:
                print("[Error] Exception during tool_use parsing:", str(e), data)

        content = remaining_text.strip()
        if not content: content = ""
        return MockResponse(thinking, content, tool_calls, text)

def tryparse(json_str):
    try: return json.loads(json_str)
    except:
        return json.loads(json_str[:-1])

if __name__ == "__main__":
    llmclient = ToolClient(LLMSession().ask)
    response = llmclient.chat(
        messages=[{"role": "user", "content": "æˆ‘çš„IPæ˜¯å¤šå°‘"}], 
        tools=[{"name": "get_ip", "parameters": {}}]
    )
    # 4. è·å–ç»“æœ
    print(f"æ€è€ƒ: {response.thinking}") 
    # -> æˆ‘éœ€è¦æŸ¥ä¸€ä¸‹ IPã€‚

    if response.tool_calls:
        cmd = response.tool_calls[0]
        print(f"è°ƒç”¨: {cmd.function.name} å‚æ•°: {cmd.function.arguments}")

    response = llmclient.chat(
        messages=[{"role": "user", "content": "<tool_result>10.176.45.12</tool_result>"}] 
    )
    print(response.content)